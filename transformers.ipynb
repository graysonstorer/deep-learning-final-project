{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "973d1b09-fa13-4bfe-8af2-e0d3c07d15d0",
   "metadata": {},
   "source": [
    "# CS6540 Deep Learning Final Project\n",
    "\n",
    "\n",
    "## Graph based Retrieval Augmented Generation using the WildGraph benchmarking system. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a16e88-7dfa-4c02-8fa0-a1992bbb656e",
   "metadata": {},
   "source": [
    "### Imports + Hardware accelerator setup for Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94172f63-8d83-489d-8069-3017e26d8426",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device  mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Configure device\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# print(f\"Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # For Apple Silicon\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "device = torch.device(device)\n",
    "\n",
    "print(\"using device \",device)\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Store results for comparison\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08030190-604e-4b48-ba00-1c481d6971c5",
   "metadata": {},
   "source": [
    "### Function definition for training the vanilla Seq2Seq model (no attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9527f4de-8b41-4a6a-91a5-b337bfa8b17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_seq2seq(model, train_loader, val_loader, epochs=10, lr=0.001, name=\"Model\", clip=1.0):\n",
    "    \"\"\"\n",
    "    Training loop for sequence-to-sequence models.\n",
    "    Returns training history for visualization including loss and accuracy.\n",
    "\n",
    "    Args:\n",
    "        model: The seq2seq model to train\n",
    "        train_loader: DataLoader for training data\n",
    "        val_loader: DataLoader for validation data\n",
    "        epochs: Number of training epochs\n",
    "        lr: Learning rate for Adam optimizer\n",
    "        name: Model name for printing progress\n",
    "        clip: Gradient clipping threshold to prevent exploding gradients\n",
    "    \"\"\"\n",
    "    # Move model to GPU if available\n",
    "    model = model.to(device)\n",
    "\n",
    "    # CrossEntropyLoss with ignore_index=0 means we don't compute loss for padding tokens\n",
    "    # This is important because padding tokens shouldn't affect our loss calculation\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    # Adam optimizer - good default choice for seq2seq models\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"\\nTraining {name} for {epochs} epochs...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Track metrics for plotting later\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # =====================\n",
    "        # TRAINING PHASE\n",
    "        # =====================\n",
    "        model.train()  # Set model to training mode (enables dropout, etc.)\n",
    "        running_loss = 0.0\n",
    "        correct_tokens = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for src, tgt, src_lens, tgt_lens in train_loader:\n",
    "            # Move data to GPU\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            # Clear gradients from previous batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # TEACHER FORCING: Feed the target sequence (shifted by 1) as input to decoder\n",
    "            # tgt[:, :-1] = all tokens except the last (input to decoder)\n",
    "            # tgt[:, 1:] = all tokens except the first (what we want to predict)\n",
    "            # Example: tgt = [<sos>, je, suis, <eos>]\n",
    "            #   Input to decoder: [<sos>, je, suis]\n",
    "            #   Target output:    [je, suis, <eos>]\n",
    "            output = model(src, tgt[:, :-1], src_lens)\n",
    "\n",
    "            # Reshape for loss calculation\n",
    "            # output: (batch, seq_len, vocab_size) -> (batch * seq_len, vocab_size)\n",
    "            # target: (batch, seq_len) -> (batch * seq_len)\n",
    "            output_flat = output.reshape(-1, output.shape[-1])\n",
    "            tgt_flat = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            # Compute cross-entropy loss\n",
    "            loss = criterion(output_flat, tgt_flat)\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # GRADIENT CLIPPING: Prevents exploding gradients in RNNs\n",
    "            # If gradient norm > clip, scale it down\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "            # Update model parameters\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Calculate token-level accuracy (excluding padding tokens)\n",
    "            predictions = output.argmax(dim=-1)  # Get predicted token indices\n",
    "            targets = tgt[:, 1:]  # Remove <sos> token\n",
    "\n",
    "            # Create mask: True for non-padding tokens, False for padding\n",
    "            non_pad_mask = targets != 0\n",
    "\n",
    "            # Count correct predictions only for non-padding tokens\n",
    "            correct_tokens += ((predictions == targets) & non_pad_mask).sum().item()\n",
    "            total_tokens += non_pad_mask.sum().item()\n",
    "\n",
    "        # Average loss over all batches\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_acc = 100 * correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "        # =====================\n",
    "        # VALIDATION PHASE\n",
    "        # =====================\n",
    "        model.eval()  # Set model to evaluation mode (disables dropout)\n",
    "        val_loss = 0.0\n",
    "        correct_tokens = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        # No gradient computation needed for validation\n",
    "        with torch.no_grad():\n",
    "            for src, tgt, src_lens, tgt_lens in val_loader:\n",
    "                src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "                # Same forward pass as training\n",
    "                output = model(src, tgt[:, :-1], src_lens)\n",
    "\n",
    "                output_flat = output.reshape(-1, output.shape[-1])\n",
    "                tgt_flat = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "                loss = criterion(output_flat, tgt_flat)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Calculate accuracy\n",
    "                predictions = output.argmax(dim=-1)\n",
    "                targets = tgt[:, 1:]\n",
    "                non_pad_mask = targets != 0\n",
    "                correct_tokens += ((predictions == targets) & non_pad_mask).sum().item()\n",
    "                total_tokens += non_pad_mask.sum().item()\n",
    "\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_acc = 100 * correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "        # Print progress\n",
    "        print(f\"  Epoch [{epoch+1}/{epochs}] | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.2f}% | Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "\n",
    "        # Store metrics for plotting\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "    duration = time.time() - start_time\n",
    "    print(f\"{name} - Final Val Loss: {history['val_loss'][-1]:.4f}, Val Acc: {history['val_acc'][-1]:.2f}%, Time: {duration:.2f}s\")\n",
    "\n",
    "    return history, duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dbcc31-0922-4382-add6-687b8e551d44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
