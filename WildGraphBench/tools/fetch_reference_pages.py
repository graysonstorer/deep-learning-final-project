#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Batch fetching of reference source pages

Features:
  Reads references.jsonl (generated by extract_references.py), fetches URLs within specified line number range,
  uses goliath and jina_scraping tools to get page content and save as Markdown.

Fetching strategy:
  - Use --fetcher to specify primary fetcher: goliath or jina
  - Use --archive-mode to control archive_url usage:
      auto   : (default) If archive_url exists, try archive first, then original url
      direct : Ignore archive_url, fetch original url directly; if low quality and archive_url exists, use archive_url as one-time fallback and re-verify
  - If primary is goliath (default, cost-optimized):
      1) In auto mode, try archive->main order; in direct mode, only try main
      2) If goliath fails with recoverable error, fallback to jina
  - If primary is jina (with retry protection):
      1) Retry jina --jina-retries times for current URL, switch to goliath only if all fail

Usage example:
  python fetch_reference_pages.py \
    --references /path/to/wiki_data/Joe Biden/reference/references.jsonl \
    --output-dir /path/to/wiki_data/Joe Biden/reference/ref_pages \
    --start 1 --end 100 --fetcher jina --jina-retries 5 --archive-mode direct
"""

from __future__ import annotations

import argparse
import json
import os
from typing import Dict, Any, List, Tuple

from jina_scraping import WebScrapingJinaTool, DEFAULT_API_KEY, slugify  # type: ignore
from goliath import build_default_goliath_tool, GoliathTool  # type: ignore


def save_markdown(
  reference: Dict[str, Any],
  fetched: Dict[str, Any],
  output_dir: str,
  line_no: int,
  skip_exists: bool
) -> str:
  """Save reference scrape result as clean Markdown."""
  os.makedirs(output_dir, exist_ok=True)
  title = reference.get('title') or fetched.get('title') or 'Untitled'
  slug = slugify(title)
  filename = f"{slug}.md"
  path = os.path.join(output_dir, filename)

  if skip_exists and os.path.exists(path):
    return path

  parts: List[str] = []
  parts.append(f"# {title}\n")

  if 'error' in fetched:
    parts.append(f"<!-- fetch error: {fetched['error']} -->\n")
  else:
    content = fetched.get('content', '').rstrip() + '\n'
    parts.append(content)

  with open(path, 'w', encoding='utf-8') as f:
    f.write('\n'.join(parts))

  return path


def load_references(jsonl_path: str) -> List[Dict[str, Any]]:
  refs: List[Dict[str, Any]] = []
  with open(jsonl_path, 'r', encoding='utf-8') as f:
    for line in f:
      line_stripped = line.strip()
      if not line_stripped:
        continue
      try:
        data = json.loads(line_stripped)
      except json.JSONDecodeError:
        data = {'error': 'json_decode_error', 'raw': line_stripped}
      refs.append(data)
  return refs


def save_references(jsonl_path: str, refs: List[Dict[str, Any]]):
  tmp_path = jsonl_path + '.tmp'
  with open(tmp_path, 'w', encoding='utf-8') as f:
    for r in refs:
      f.write(json.dumps(r, ensure_ascii=False) + '\n')
  os.replace(tmp_path, jsonl_path)


def is_low_quality(content: str) -> Tuple[bool, str]:
  """Determine if content is a low-quality page that should be filtered.

  Rules:
    1. Contains known 404/error page signature strings
    2. Total lines < 5 and each line has < 100 words (more relaxed: lines < 10 and all lines < 50 words)

  Returns (should_filter, reason)
  """
  if not content:
    return True, 'empty_content'
  patterns = [
    '404 | Fox News',
    'It seems you clicked on a bad link',
    'Something has gone wrong',
    '404 Not Found',
    'Page Not Found',
    'Access Denied',
    'Permission Denied',
  ]
  for p in patterns:
    if p in content:
      return True, f'match_pattern:{p}'

  # Relaxed low-quality check
  lines = [l.strip() for l in content.splitlines() if l.strip()]
  if len(lines) < 13:
    all_short = True
    for line in lines:
      if len(line.split()) >= 50:
        all_short = False
        break
    if all_short:
      return True, 'line_count<10_and_all_lines<50words'
  return False, ''


def is_goliath_error(error_msg: str) -> bool:
  """Determine if this is a common goliath error type that should retry with jina"""
  if not error_msg:
    return False
  goliath_error_patterns = [
    'timeout', 'connection', 'network', 'selenium', 'browser',
    'webdriver', 'javascript', 'render', 'chrome', 'proxy',
    # Can extend as needed: '403', '429', 'captcha', 'forbidden', 'cloudflare', 'blocked', 'bot'
  ]
  error_lower = error_msg.lower()
  return any(pattern in error_lower for pattern in goliath_error_patterns)


def main():
  parser = argparse.ArgumentParser(
    description='Batch fetch URLs from references.jsonl and save as Markdown'
  )
  parser.add_argument(
    '--references',
    required=True,
    help='Path to references.jsonl'
  )
  parser.add_argument(
    '--output-dir',
    required=True,
    help='Output directory for fetched reference page Markdown files'
  )
  parser.add_argument(
    '--start',
    type=int,
    default=1,
    help='Start line number (1-based)'
  )
  parser.add_argument(
    '--end',
    type=int,
    default=None,
    help='End line number (1-based, inclusive)'
  )
  parser.add_argument(
    '--api-key',
    dest='api_key',
    default=None,
    help='Jina API Key (used for fallback or as primary when specified)'
  )
  parser.add_argument(
    '--fetcher',
    choices=['goliath', 'jina'],
    default='goliath',
    help='Primary fetcher: goliath (default) or jina'
  )
  parser.add_argument(
    '--jina-retries',
    type=int,
    default=5,
    help='When --fetcher=jina, consecutive retries for same URL before switching to goliath'
  )
  parser.add_argument(
    '--archive-mode',
    choices=['auto', 'direct'],
    default='direct',
    help='auto=try archive then main; direct=ignore archive_url, fetch original url; if low quality, try archive as fallback'
  )
  parser.add_argument(
    '--skip-exists',
    dest='skip_exists',
    action='store_true',
    default=True,
    help='Skip if file already exists (default)'
  )
  parser.add_argument(
    '--no-skip-exists',
    dest='skip_exists',
    action='store_false',
    help='Overwrite even if exists'
  )
  parser.add_argument(
    '--verbose',
    action='store_true',
    help='Output detailed fetch attempt logs'
  )
  parser.add_argument(
    '--force',
    action='store_true',
    help='Ignore scraped flag, force re-fetch and overwrite fetcher_used'
  )
  parser.add_argument(
    '--record-attempts',
    action='store_true',
    help='Write attempt_log detailed results to reference entries'
  )
  args = parser.parse_args()

  primary = args.fetcher            # 'goliath' or 'jina'
  archive_mode = args.archive_mode  # 'auto' or 'direct'

  # Prepare tools
  g_tool = build_default_goliath_tool()

  api_key = args.api_key or os.environ.get('JINA_API_KEY') or DEFAULT_API_KEY
  if not api_key.startswith('Bearer '):
    api_key = f'Bearer {api_key}'
  jina_tool = WebScrapingJinaTool(api_key)

  def normalize(data: dict) -> dict:
    return {
      'title': data.get('title', ''),
      'content': data.get('content', ''),
      'publish_time': data.get('publish_time', '') or data.get('fetched_publish_time', '')
    }

  def try_goliath(url: str):
    try:
      d = g_tool(url)
      if not d.get('success') or d.get('error'):
        return None, d.get('error', 'goliath_error')
      return normalize(d), None
    except Exception as e:
      return None, str(e)

  def try_jina(url: str):
    try:
      d = jina_tool(url)
      if 'error' in d:
        return None, d.get('error', 'jina_error')
      return normalize(d), None
    except Exception as e:
      return None, str(e)

  def try_jina_with_retries(
    url: str,
    url_type: str,
    retries: int
  ) -> Tuple[Any, Any, List[Tuple[str, str, str, bool, str]]]:
    attempts_local: List[Tuple[str, str, str, bool, str]] = []
    last_err = None
    for i in range(1, retries + 1):
      if args.verbose:
        print(f"       Trying jina({url_type}) attempt {i}/{retries}")
      norm, err = try_jina(url)
      attempts_local.append(
        (f'jina_{url_type}_try{i}', 'jina', url, err is None, '' if err is None else err)
      )
      if err is None:
        return norm, None, attempts_local
      last_err = err
    return None, last_err, attempts_local

  # NEW: Extract common logic for "single URL fetch" to support direct mode fallback
  def fetch_one(url_type: str, url_to_try: str):
    """Fetch single URL with current primary/retry strategy, returns (norm, used_label, attempts)"""
    attempts: List[Tuple[str, str, str, bool, str]] = []
    if primary == 'goliath':
      if args.verbose:
        print(f"       Trying goliath({url_type})")
      norm, err = try_goliath(url_to_try)
      attempts.append(
        (f'goliath_{url_type}', 'goliath', url_to_try, err is None, '' if err is None else err)
      )
      if err is None:
        used_label = f'goliath({url_type})' if url_type == 'archive' else 'goliath'
        return norm, used_label, attempts
      if is_goliath_error(err):
        if args.verbose:
          print(f"       goliath failed ({err}), fallback to jina({url_type})")
        norm_j, err_j = try_jina(url_to_try)
        attempts.append(
          (f'jina_{url_type}', 'jina', url_to_try, err_j is None, '' if err_j is None else err_j)
        )
        if err_j is None:
          used_label = f'jina({url_type})' if url_type == 'archive' else 'jina'
          return norm_j, used_label, attempts
    else:
      norm_j, err_j, j_attempts = try_jina_with_retries(url_to_try, url_type, args.jina_retries)
      attempts.extend(j_attempts)
      if err_j is None:
        used_label = f'jina({url_type})' if url_type == 'archive' else 'jina'
        return norm_j, used_label, attempts
      if args.verbose:
        print(f"       jina failed {args.jina_retries} times consecutively, switching to goliath({url_type})")
      norm_g, err_g = try_goliath(url_to_try)
      attempts.append(
        (f'goliath_{url_type}', 'goliath', url_to_try, err_g is None, '' if err_g is None else err_g)
      )
      if err_g is None:
        used_label = f'goliath({url_type})' if url_type == 'archive' else 'goliath'
        return norm_g, used_label, attempts
    return None, 'failed', attempts

  def fetch_with_fallback(ref: dict):
    """Execute fetch strategy based on primary fetcher and archive_mode, returns: (norm, used_label, attempts)"""
    url_main = ref.get('url')
    archive_url = ref.get('archive_url')
    attempts_all: List[Tuple[str, str, str, bool, str]] = []

    # URL fetch sequence
    if archive_mode == 'auto' and archive_url:
      sequence: List[Tuple[str, str]] = [('archive', archive_url), ('main', url_main)]
    else:
      sequence = [('main', url_main)]

    for url_type, url_to_try in sequence:
      if args.verbose:
        print(f"    -> Processing {url_type} url: {url_to_try}")
      norm, used_label, attempts = fetch_one(url_type, url_to_try)
      attempts_all.extend(attempts)
      if norm is not None:
        return norm, used_label, attempts_all

    return None, 'failed', attempts_all

  refs = load_references(args.references)
  total = 0
  success = 0
  failed = 0
  start = args.start if args.start >= 1 else 1
  end = args.end if args.end is not None else len(refs)
  end = min(end, len(refs))

  # Fallback statistics
  fallback_count = 0
  goliath_error_count = 0

  print(
    f"[INFO] Fetch strategy: primary={primary} ("
    f"{'cost-priority' if primary == 'goliath' else f'Jina retries {args.jina_retries} times then switches to Goliath'})"
  )
  print(f"[INFO] Archive mode: {archive_mode}")
  print(f"[INFO] Processing range: {start}-{end} (total {end - start + 1})")

  for idx in range(start, end + 1):  # idx is 1-based line number
    ref = refs[idx - 1]
    total += 1
    url = ref.get('url')
    archive_url = ref.get('archive_url')
    if not url:
      failed += 1
      print(f"[line {idx}] Missing url, skipping")
      continue
    if ref.get('scraped') is True and not args.force:
      print(f"[line {idx}] Already scraped, skipping (use --force to re-fetch)")
      continue

    print(f"[line {idx}] Fetching {url} ...")
    max_filter_retry = 3
    filter_attempts = 0
    aggregate_attempt_log: List[Dict[str, Any]] = []
    tried_archive_fill = False  # NEW: direct mode "archive fallback" only done once

    while True:
      norm, used, attempts = fetch_with_fallback(ref)

      # Track whether fallback occurred and whether there was a goliath error
      used_fallback = any((a[1] != primary) and a[3] for a in attempts)
      had_goliath_error = any((a[1] == 'goliath') and (not a[3]) for a in attempts)
      if had_goliath_error:
        goliath_error_count += 1
      if used_fallback:
        fallback_count += 1

      if norm is None:
        failed += 1
        if args.record_attempts:
          aggregate_attempt_log.extend([
            {
              'round': filter_attempts + 1,
              'step': a[0],
              'engine': a[1],
              'url': a[2],
              'success': a[3],
              'error': a[4],
            }
            for a in attempts
          ])
          ref['attempt_log'] = aggregate_attempt_log
        err_msg = '; '.join(f"{a[0]}:{a[4]}" for a in attempts if a[4]) or 'unknown_error'
        print(f"  Failed(fetcher_used={used}): {err_msg} (file not saved)")
        break

      if args.record_attempts:
        aggregate_attempt_log.extend([
          {
            'round': filter_attempts + 1,
            'step': a[0],
            'engine': a[1],
            'url': a[2],
            'success': a[3],
            'error': a[4],
          }
          for a in attempts
        ])

      # Quality validation
      filt, reason = is_low_quality(norm.get('content', ''))
      if filt:
        # NEW: In direct mode, if archive_url exists and archive fallback hasn't been tried yet, fetch archive and validate
        if (archive_mode == 'direct') and archive_url and (not tried_archive_fill):
          tried_archive_fill = True
          print("  Low quality content, direct mode activating archive fallback -> fetching archive_url to verify")
          norm2, used2, attempts2 = fetch_one('archive', archive_url)

          # Statistics/recording
          used_fallback2 = any((a[1] != primary) and a[3] for a in attempts2)
          had_goliath_error2 = any((a[1] == 'goliath') and (not a[3]) for a in attempts2)
          if had_goliath_error2:
            goliath_error_count += 1
          if used_fallback2:
            fallback_count += 1
          if args.record_attempts:
            aggregate_attempt_log.extend([
              {
                'round': filter_attempts + 1,
                'step': a[0],
                'engine': a[1],
                'url': a[2],
                'success': a[3],
                'error': a[4],
              }
              for a in attempts2
            ])

          if norm2 is None:
            # Archive fetch failed, mark as processed but don't save
            ref['scraped'] = True
            ref['fetcher_used'] = used + '->archive(fetch_failed)'
            ref['filter_reason'] = f'direct_low_quality_then_archive_fetch_failed:{reason}'
            success += 1
            print("  Archive fallback fetch failed, marking scraped=true without saving file")
            break

          # Quality validation for archive content
          filt2, reason2 = is_low_quality(norm2.get('content', ''))
          if filt2:
            ref['scraped'] = True
            ref['fetcher_used'] = used2 + '(low_quality)'
            ref['filter_reason'] = f'direct_low_quality_then_archive_low_quality:{reason2}'
            success += 1
            print(f"  Archive content still low quality({reason2}), marking scraped=true without saving file")
            break

          # Archive content acceptable -> save and complete
          success += 1
          ref['scraped'] = True
          ref['fetcher_used'] = used2
          if args.record_attempts:
            ref['attempt_log'] = aggregate_attempt_log
          out_path = save_markdown(ref, norm2, args.output_dir, idx, args.skip_exists)
          print(f"  Saved -> {out_path} (fetcher_used={used2}) [direct mode archive fallback success]")
          break

        # Non-direct mode or archive fallback already tried: use original retry limit
        filter_attempts += 1
        print(f"  Low quality content(reason={reason}) attempt {filter_attempts}")
        if filter_attempts < max_filter_retry:
          print("  -> Retrying fetch...")
          continue
        else:
          ref['scraped'] = True
          ref['fetcher_used'] = used + '(low_quality)' if used else 'low_quality'
          ref['filter_reason'] = reason
          if args.record_attempts:
            ref['attempt_log'] = aggregate_attempt_log
          success += 1
          print(f"  Max retries reached, marking scraped=true without saving file")
          break

      else:
        # Content acceptable, save
        success += 1
        ref['scraped'] = True
        ref['fetcher_used'] = used
        if args.record_attempts:
          ref['attempt_log'] = aggregate_attempt_log
        out_path = save_markdown(ref, norm, args.output_dir, idx, args.skip_exists)
        fallback_msg = ""
        if used_fallback:
          fallback_msg = " [FALLBACK SUCCESS]"
        elif had_goliath_error and not used_fallback:
          fallback_msg = " [GOLIATH ERROR BUT FALLBACK FAILED]"
        print(f"  Saved -> {out_path} (fetcher_used={used}){fallback_msg}")
        break

    if total % 10 == 0:
      save_references(args.references, refs)

  save_references(args.references, refs)

  print(f"\nComplete: total={total} success={success} failed={failed}")
  print(f"Fallback stats: goliath_errors={goliath_error_count}, fallback_success={fallback_count}")

  from collections import Counter
  used_counter = Counter(r.get('fetcher_used') for r in refs if r.get('scraped'))
  if used_counter:
    print("\nSuccess source statistics:")
    for k, v in used_counter.most_common():
      if k:
        print(f"  {k}: {v}")


if __name__ == '__main__':
  main()
